[{"authors":["**Pilhyeon Lee**","Jinglu Wang","Yan Lu","Hyeran Byun"],"categories":[],"content":"","date":1612234800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612234800,"objectID":"2bebb22a53eb26d29878c55e22d49ff0","permalink":"https://pilhyeon.github.io/publication/2021_aaai_uncertainty-modeling/","publishdate":"2020-12-02T12:00:00+09:00","relpermalink":"/publication/2021_aaai_uncertainty-modeling/","section":"publication","summary":"Weakly-supervised temporal action localization aims to detect intervals of action instances with only video-level action labels for training. A crucial challenge is to separate frames of action classes from remaining, denoted as background frames (i.e., frames not belonging to any action class). Previous methods attempt background modeling by either synthesizing pseudo background videos with static frames or introducing an auxiliary class for background. However, they overlook an essential fact that background frames could be dynamic and inconsistent. Accordingly, we cast the problem of identifying background frames as out-of-distribution detection and isolate it from conventional action classification. Beyond our base action localization network, we propose a module to estimate the probability of being background (i.e., uncertainty), which allows us to learn uncertainty given only video-level labels via multiple instance learning. A background entropy loss is further designed to reject background frames by forcing them to have uniform probability distribution for action classes. Extensive experiments verify the effectiveness of our background modeling and show that our method significantly outperforms state-of-the-art methods on the standard benchmarks - THUMOS'14 and ActivityNet (1.2 and 1.3). Our code and the trained model are available at [https://github.com/Pilhyeon/Background-Modeling-via-Uncertainty-Estimation](https://github.com/Pilhyeon/Background-Modeling-via-Uncertainty-Estimation).","tags":[],"title":"Weakly-supervised Temporal Action Localization by Uncertainty Modeling","type":"publication"},{"authors":["Sunhee Hwang$^*$","Sungho Park$^*$","**Pilhyeon Lee**$^*$","Seogkyu Jeon","Dohyung Kim","Hyeran Byun"],"categories":[],"content":"","date":1606705200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606705200,"objectID":"d1ba2fb223a79f5ba243c3dde4f92356","permalink":"https://pilhyeon.github.io/publication/2020_accv_transferable-fairness/","publishdate":"2020-11-30T12:00:00+09:00","relpermalink":"/publication/2020_accv_transferable-fairness/","section":"publication","summary":"Recent studies have revealed the importance of fairness in machine learning and computer vision systems, in accordance with the concerns about the unintended social discrimination produced by the systems. In this work, we aim to tackle the fairness-aware image classification problem, whose goal is to classify a target attribute (eg, attractiveness) in a fair manner regarding protected attributes (eg, gender, age, race). To this end, existing methods mainly rely on protected attribute labels for training, which are costly and sometimes unavailable for real-world scenarios. To alleviate the restriction and enlarge the scalability of fair models, we introduce a new framework where a fair classification model can be trained on datasets without protected attribute labels (ie, target datasets) by exploiting knowledge from pre-built benchmarks (ie, source datasets). Specifically, when training a target attribute encoder, we encourage its representations to be independent of the features from the pre-trained encoder on a source dataset. Moreover, we design a Group-wise Fair loss to minimize the gap in error rates between different protected attribute groups. To the best of our knowledge, this work is the first attempt to train the fairness-aware image classification model on a target dataset without protected attribute annotations. To verify the effectiveness of our approach, we conduct experiments on CelebA and UTK datasets with two settings: the conventional and the transfer settings. In the both settings, our model shows the fairest results when compared to the existing methods.","tags":[],"title":"Exploiting Transferable Knowledge for Fairness-aware Image Classification","type":"publication"},{"authors":["**Pilhyeon Lee**","Youngjung Uh","Hyeran Byun"],"categories":[],"content":"","date":1581044400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581044400,"objectID":"15bd45477cb63bcf6d826ccf162fd25b","permalink":"https://pilhyeon.github.io/publication/2020_aaai_bas-net/","publishdate":"2020-02-07T12:00:00+09:00","relpermalink":"/publication/2020_aaai_bas-net/","section":"publication","summary":"Weakly-supervised temporal action localization is a very challenging problem because frame-wise labels are not given in the training stage while the only hint is video-level labels: whether each video contains action frames of interest. Previous methods aggregate frame-level class scores to produce video-level prediction and learn from video-level action labels. This formulation does not fully model the problem in that background frames are forced to be misclassified as action classes to predict video-level labels accurately. In this paper, we design Background Suppression Network (BaS-Net) which introduces an auxiliary class for background and has a two-branch weight-sharing architecture with an asymmetrical training strategy. This enables BaS-Net to suppress activations from background frames to improve localization performance. Extensive experiments demonstrate the effectiveness of BaS-Net and its superiority over the state-of-the-art methods on the most popular benchmarks - THUMOS'14 and ActivityNet. Our code and the trained model are available at [https://github.com/Pilhyeon/BaSNet-pytorch](https://github.com/Pilhyeon/BaSNet-pytorch).","tags":[],"title":"Background Suppression Network for Weakly-supervised Temporal Action Localization","type":"publication"}]